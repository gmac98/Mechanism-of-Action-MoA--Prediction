{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfEYSQTyMrq6",
        "outputId": "1e9060a6-16ac-430c-fc9e-7202cf55d822"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2.2, -1.4, -.8, .2, .4, .8, 1.2, 2.2, 2.9, 4.6])\n",
        "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
        "\n",
        "logr = LogisticRegression(solver='lbfgs')\n",
        "logr.fit(x.reshape(-1, 1), y)\n",
        "\n",
        "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
        "loss = log_loss(y, y_pred)\n",
        "\n",
        "print('x = {}'.format(x))\n",
        "print('y = {}'.format(y))\n",
        "print('p(y) = {}'.format(np.round(y_pred, 2)))\n",
        "print('Log Loss / Cross Entropy = {:.4f}'.format(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = [-2.2 -1.4 -0.8  0.2  0.4  0.8  1.2  2.2  2.9  4.6]\n",
            "y = [0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            "p(y) = [0.19 0.33 0.47 0.7  0.74 0.81 0.86 0.94 0.97 0.99]\n",
            "Log Loss / Cross Entropy = 0.3329\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}